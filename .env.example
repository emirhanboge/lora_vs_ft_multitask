# Base paths - Customize these for your environment
DATA_DIR=data/llama_1b
MODEL_DIR=models/llama_1b
LOGS_DIR=logs/llama_1b

# Model configuration
MODEL_NAME=meta-llama/Llama-3.2-1B
TOKENIZER_PATH=${DATA_DIR}/tokenizer

# Training configuration
LORA_RANK=16
LORA_ALPHA=32  # LORA_RANK * 2

# Task configurations - DO NOT MODIFY unless you know what you're doing
SIMILAR_TASKS=["sst2", "mnli", "qqp"]
DISSIMILAR_TASKS=["rajpurkar/squad_v2", "cnn_dailymail", "codex_glue"]
FORGETTING_BENCHMARKS=["hellaswag", "winogrande_l", "ai2_arc"]

# Maximum sequence lengths - Adjust based on your GPU memory
MAX_LENGTHS={
    "mnli": 128,
    "qqp": 128,
    "sst2": 128,
    "squad_v2": 512,
    "cnn_dailymail": 2048,
    "codex_glue": 1024
}

# Results and evaluation
RESULTS_FILE=${MODEL_DIR}/evaluation_results.txt

# Hugging Face configuration - ADD YOUR CREDENTIALS
HF_USERNAME=your_username_here
HF_TOKEN=your_token_here  # Get this from https://huggingface.co/settings/tokens

# Task-specific configurations - DO NOT MODIFY
TASK_LABELS={
    "sst2": 2,
    "mnli": 3,
    "qqp": 2
}

# Dataset paths
MULTI_TASK_PATH=${DATA_DIR}/sst2_mnli_qqp
QA_CODE_SUMM_PATH=${DATA_DIR}/qa_code_summarization 